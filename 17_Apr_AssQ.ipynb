{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Gradient Boosting Regression is a popular machine learning algorithm for regression problems that involves building a series of decision trees in a sequential manner. It works by combining several weak learners (decision trees) to form a strong learner (ensemble model) that can make accurate predictions. \n",
    "\n",
    ">The algorithm starts by building a single decision tree to make predictions on the training data. The errors (residuals) from this initial model are then used to train a second decision tree. This process is repeated, with each subsequent tree being trained on the errors of the previous tree, until the specified number of trees is built or a stopping criterion is met.\n",
    "\n",
    ">In each iteration, the algorithm tries to find the optimal split in the data by minimizing the residual error of the previous iteration. This is done using gradient descent, which calculates the negative gradient of the loss function with respect to the predicted values. The predicted values are then updated by adding a fraction of the gradient to them, and the process is repeated until the error is minimized.\n",
    "\n",
    ">By combining the predictions of multiple decision trees, Gradient Boosting Regression is able to capture complex nonlinear relationships between the input features and the target variable, resulting in high predictive accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.8337716869669473\n",
      "R-squared: 0.3329673595157666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        residuals = y.copy()\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            predictions = tree.predict(X)\n",
    "            self.estimators.append(tree)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "\n",
    "        for tree in self.estimators:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# load tips dataset and perform one-hot encoding\n",
    "tips = sns.load_dataset('tips')\n",
    "tips_encoded = pd.get_dummies(tips, columns=['sex', 'smoker', 'day', 'time'])\n",
    "\n",
    "X = tips_encoded.drop('tip', axis=1)\n",
    "y = tips_encoded['tip']\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test set\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# evaluate model performance using mean squared error and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best Score: 0.31534878154263496\n",
      "Mean Squared Error: 0.8654454588043873\n",
      "R-squared: 0.3076277611663921\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Tips dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "\n",
    "# Perform Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X['sex'] = le.fit_transform(X['sex'])\n",
    "X['smoker'] = le.fit_transform(X['smoker'])\n",
    "X['day'] = le.fit_transform(X['day'])\n",
    "X['time'] = le.fit_transform(X['time'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a Gradient Boosting regressor object\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Make predictions on test set using best model\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Evaluate model performance using mean squared error and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A weak learner in Gradient Boosting is a machine learning model that performs slightly better than random guessing, but is not a strong enough predictor on its own. In Gradient Boosting, a weak learner is typically a decision tree with a small number of leaves or depth. Weak learners are combined in an iterative fashion to create a strong predictor. By combining many weak learners, the final model becomes a strong learner, capable of making accurate predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The intuition behind the Gradient Boosting algorithm is to iteratively improve the predictions of a weak learner by combining its predictions with the negative gradients of a loss function. The idea is to fit the negative gradients of the loss function to the next weak learner to create a new model that improves upon the previous model. This process is repeated many times until a certain stopping criterion is met, such as the number of iterations or the convergence of the loss function. By combining multiple weak learners, Gradient Boosting creates a strong ensemble model that can make accurate predictions on new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gradient Boosting builds an ensemble of weak learners in a stage-wise manner. At each stage, it trains a new weak learner to correct the errors made by the previous weak learners in the ensemble. The idea is to minimize a loss function by adding a weak learner at each stage that reduces the residual error from the previous stage.\n",
    "\n",
    "> The algorithm starts by initializing a model with a constant value, such as the mean or median of the target variable. Then, at each iteration, a weak learner, typically a decision tree with a small depth, is trained on the negative gradient of the loss function with respect to the current model's predictions. The weak learner is trained to fit the residual errors of the current model. The learning rate controls the contribution of each weak learner to the final model. The smaller the learning rate, the slower the algorithm learns, but the more robust it is to overfitting.\n",
    "\n",
    "> The predictions of the new weak learner are then added to the predictions of the current model, and the process is repeated until a stopping criterion is met, such as reaching a maximum number of iterations, reaching a minimum improvement in the loss function, or achieving a desired performance metric on a validation set. The final model is an ensemble of weak learners, where each weak learner corrects the errors made by the previous weak learners in the ensemble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The mathematical intuition of Gradient Boosting algorithm can be broken down into the following steps:\n",
    "\n",
    "1. Initialize the model with a constant value, such as the mean of the target variable.\n",
    "\n",
    "2. Train a weak learner on the training data and use it to predict the target variable.\n",
    "\n",
    "3. Calculate the residuals between the predicted and actual values of the target variable.\n",
    "\n",
    "4. Train another weak learner on the residuals and use it to predict the residuals.\n",
    "\n",
    "5. Update the model by adding the prediction of the second weak learner to the predictions of the first weak learner.\n",
    "\n",
    "6. Repeat steps 3-5 until the desired number of weak learners is reached or until the residuals cannot be further reduced.\n",
    "\n",
    "7. The final prediction of the model is the sum of the predictions of all the weak learners."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
